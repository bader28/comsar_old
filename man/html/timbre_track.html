
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Timbre Track &#8212; COMSAR 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Pitch Track" href="pitch_track.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="timbre-track">
<h1>Timbre Track<a class="headerlink" href="#timbre-track" title="Permalink to this headline">¶</a></h1>
<p>All timbre features use a Fourier to analyze the sound in adjacent, non-overlapping frames. The analyzed discrete spectrum <span class="math notranslate nohighlight">\(A_i\)</span> with amplitude A and N frequency entries <span class="math notranslate nohighlight">\(f_i\)</span> is then further processed in the the four features. Here i is used as the vector bins, which map into frequency values through the sample frequency s and the analysis window length l in samples like :math`f_i` = i / (l / s).</p>
<ol class="arabic simple">
<li><p>Spectral Centroid</p></li>
</ol>
<p>The spectral centroid C is the center of a spectrum, where the sum of amplitudes of frequencies above and below this center are equal, and is calculated as</p>
<div class="math notranslate nohighlight">
\[C = \frac{\sum_{i=0}^N f_i A_i}{\sum_{i=0}^N A_i} \ .\]</div>
<p>This corresponds to psychoacoustic brightness perception.</p>
<ol class="arabic simple" start="2">
<li><p>Roughness</p></li>
</ol>
<p>Roughness calculations have been suggested in several ways (for a review see (Schneider2009), (Bader2013)). Basically two algorithms exist, calculating beating of two sinusoidals close to each other (Helmholtz ({Helmholtz1863), Helmholtz/Bader (Schneider2009), Sethares (Sethares1993), or integrating energy in critical bands on the cochlear (Fastl (Zwicker1999), Sothek (Sothek1994). The former has been found to work very well with musical sounds, the latter with industrial noise.</p>
<p>In the paper a modified Helmholt/Bader algorithm is used. Like Helmholtz it assumes a maximum roughness of two sinusoidals at 33 Hz frequency difference. As Helmholtz did not give a mathematical formula how he did calculate roughness, according to his verbal descriptions a curve of the amount of roughness $R_n$ is assumed between two frequencies with distance $df_n$ which have amplitudes $A_1$ and $A_2$ like</p>
<div class="math notranslate nohighlight">
\[R_n = A_1 A_2 \frac{|df_n|}{f_r e^{-1}} e^{- |df_n|/f_r} \ .\]</div>
<p>with a maximum roughness at $f_r$ = 33 Hz. The roughness R is then calculated as the sum of all possible sinusoidal combinatins like</p>
<div class="math notranslate nohighlight">
\[R = \sum_{i=1}^N R_i \ .\]</div>
<p>The only difference between the algorithm used in apollon and that described in (Schneider2009) is the precision with which the frequencies are calculated. To arrive at very precise values in (Schneider2009) a wavelet analysis is performed, allowing for an arbitrary precision of frequency estimation. As this is very expensive in terms of computational time, in the present study the above described Fourier analysis precision is used. In (Schneider2009) the research aim was to tell the perceptual differences between tuning systems like Pure Tone, Werkmeister, Kirnberger, etc. in a Baroque piece of J.S. Bach which succeeded. The present analysis is not aiming for such subtle differences, but for the overall estimation of roughness.</p>
<ol class="arabic simple" start="3">
<li><p>Sharpness</p></li>
</ol>
<p>Perceptual sharpness is related to the work of Bismarck (Bismarck1974) and followers (Aures1985b, Fastl2007). It corresponds to small frequency-band energy. According to (Fastl2007) it is measured in acum, where 1 acum is a small-band noise within one critical band around 1 kHz at 60 dB loudness level. Sharpness increases with frequency in a nonlinear way. If a small-band noise increases its center frequency from about 200 Hz to 3 kHz sharpness increases slightly, but above 3 kHz strongly, according to perception that very high small-band sounds have strong sharpness. Still sharpness is mostly independent of overall loudness, spectral centroid, or roughness, and therefore qualifies as a parameter on its own.</p>
<p>To calculate sharpness the spectrum A is integrated with respect to 24 critical or Bark bands, as we are considering small-band noise. With loudness $L_B$ at each Bark band B sharpness is</p>
<div class="math notranslate nohighlight">
\[S = 0.11 \frac{\sum_{B=0}^{24 Bark} L_B g_B B}{\sum_{B=0}^{24 Bark} L_B} \ \text{acum} ,\]</div>
<p>where a weighting function $g_B$ is used strengthening sharpness above 3 kHz like({Peeters2004}</p>
<div class="math notranslate nohighlight">
\[\begin{split}g_B = \left\{\begin{array}{ll} 1 \text{ if} B &lt; 15 \\ 0.066 e^{0.171 B} \text{ if} z \geq 15 \end{array} \right.\end{split}\]</div>
<ol class="arabic simple" start="4">
<li><p>Loudness</p></li>
</ol>
<p>Although several algorithms of sound loudness have been proposed (Fastl2007), for music still no satisfying results have been obtained (Rusckowski2013). Most loudness algorithms aim for industrial noise and it appears that musical content considerably contributes to perceived loudness. Also loudness is found to statistically significantly differ between male and female subjects due to the different constructions of the outer ears between the sexes. Therefore a very simple estimation of loudness is used, and further investigations in the subject are needed. The algorithm used is</p>
<div class="math notranslate nohighlight">
\[L = 20 \log_{10} \frac{1}{N}\sqrt{\sum_{i=0}^N \frac{A_i^2}{A_{ref}^2}} \ .\]</div>
<p>This corresponds to the definition of decibel, using a rough logarithm-of-ten compression according to perception, and a multiplication with 20 to arrive at 120 dB for a sound pressure level of about 1 Pa. Of course the digital audio data are not physical sound pressure levels (SPL), still the algorithm is used to obtain dB-values most readers are used to. As all psychoacoustic parameters are normalized before inputting them into the SOM, the absolute value is not relevant.</p>
<ol class="arabic simple" start="5">
<li><p>Fractal Correlation Dimension</p></li>
</ol>
<p>The fractal correlation dimensions measures chaoticity. Chaoticity is defined the the amount of harmonic overtone spectra plus large amplitude fluctuations. So a single guitar tone in its steady-state has a fractal correlation dimension of one. A piano chord with three keys pressed has a fractal dimension of three.  Each inharmonic sinusoidal added another dimension. White noise therefore has a dimension of infinity.</p>
<p>To calculate a fractal correlation dimension, a time series x(t) of recorded sound with n sample points is embedded in a pseudo phase-plot like</p>
<div class="math notranslate nohighlight">
\[X(t) = \{x(t),x(t+\delta), x(t+ 2\delta), ..., x(t+d \delta)\}\ .\]</div>
<p>Starting from X(t) we then calculate the ‘area’ or ‘volume’ C(r) like</p>
<div class="math notranslate nohighlight">
\[C(r) = \frac{1}{N^2} \sum_{i \neq j} H(r-|\mathbf{X}_i -\mathbf{X}_j|) \ .\]</div>
<p>Here, H(x) is the Heavyside function with</p>
<div class="math notranslate nohighlight">
\[\begin{split}H(x) = \left\{%
\begin{array}{ll}
  0, &amp; \hbox{for x} \leq 0 \\
  1, &amp; \hbox{for x &gt; 0} \\
\end{array}%
\right.\end{split}\]</div>
<p>which counts the amount of points within the radius r. C(r) is a probability, as we normalize with respect to all <span class="math notranslate nohighlight">\(N^2\)</span> possible combinations.</p>
<p>Then the correlation dimension is defined as</p>
<div class="math notranslate nohighlight">
\[D_C = \lim_{r \rightarrow 0} \frac{\ln C(r)}{\ln r} \ ,\]</div>
<p>which is derived from the idea of the definition of the dimension (Bader2013). The exponent is the dimension which is the slope of a log-log plot. So practically we need to take the middle of the distribution and look for a constant slope in the log-log plot. This slope is the correlation dimension.</p>
<p>This is a very powerful tool as it has certain properties:</p>
<ol class="arabic simple">
<li><p>If only one harmonic overtone spectrum is in the sound, DC = 1 no matter how many overtones are present.</p></li>
<li><p>Each additional harmonic overtone spectrum raises DC to the next integer.</p></li>
<li><p>If only one inharmonic sinusodial is added, DC raises to the next integer making it suitable for detection of additional single inharmonic components too.</p></li>
<li><p>Large amplitude fluctuations lead to a raise of DC.</p></li>
<li><p>As the absolute amplitude is normalized, DC does not depend upon amplitude.</p></li>
<li><p>If a component is below a certain amplitude threshold it is no longer considered by the algorithm.</p></li>
</ol>
<p>The fractal correlation dimension raises with initial transients, as they contain chaoticity. It is also a good measure of event density in a musical piece.</p>
<ol class="arabic simple" start="6">
<li><p>Correlogram</p></li>
</ol>
<p>The correlogram is the opposite to the fractal dimensin. There the chaoticity is calculated, a correlogram calculates the harmonicity. While with very complex sounds the fractal dimension is at a maximum, with these sounds the correlogram becomes nearly zero.</p>
<p>It is calculated for a time series y(t)  for time points t and frequency f = i / S with sample frequency S and running sample delay i like</p>
<div class="math notranslate nohighlight">
\[C(t, i) = \frac{A_{t,i}^M}{\sqrt{B_t^M C_{t,i}^M}} \ .\]</div>
<p>Here <span class="math notranslate nohighlight">\(A_{t,i}^M\)</span> is the autocorrelation sum</p>
<div class="math notranslate nohighlight">
\[A_{t,i}^M = \sum_{j=0}^M y(t+j) y(t+i+j)\]</div>
<p>at point t for sample delay i, corresponding to a frequency, over
an autocorrelation length M. It is normalized by</p>
<div class="math notranslate nohighlight">
\[B_t^M = \sum_{j=0}^M y(t+j)^2 \ ,\]</div>
<p>as the squared length of the time series vector over M starting at t, and by</p>
<div class="math notranslate nohighlight">
\[C_{t,i}^M = \sum_{j=0}^M y(t+i+j)^2 \ ,\]</div>
<p>as the squared sum of the time series vector over M starting from t + i. So after inserting A, B, and C we have</p>
<div class="math notranslate nohighlight">
\[C(t, i) = \frac{\sum_{j=0}^M y(t+j) y(t+i+j)}{\sqrt{\sum_{j=0}^M y(t+j)^2 \sum_{j=0}^M y(t+i+j)^2}} \ .\]</div>
<p>The correlogram is equivalent to an autocorrelation of an autocorrelation. The first autocorrelation detects periodicities within the sound. These periodicity series are again used as the input of another autocorrelation. So by applying this convolution integral twice, we detect not single frequencies but periodicities of frequencies, harmonic series.</p>
<ol class="arabic simple" start="7">
<li><p>Spread</p></li>
</ol>
<p>The spectral spread is defined as the standard deviation of a spectrum around its mean like</p>
<div class="math notranslate nohighlight">
\[Sp = \sqrt{\left(\sum_{i=0}^N (f_i - C)^2\ A_i\right)}\]</div>
<p>with spectral centroid C. Broadband spectra therefore have a large spread, while narroband signals are low with respect to this measure.</p>
<ol class="arabic simple" start="8">
<li><p>Skewness</p></li>
</ol>
<p>Spectral skewness calculates the assymetry around the spectral centroid. If more energy is above C than below, Sk &gt; 0.</p>
<div class="math notranslate nohighlight">
\[Sk = \sqrt{\left(\sum_{i=0}^N (f_i - C)^3\ A_i\right)}/Sp^3\]</div>
<ol class="arabic simple" start="9">
<li><p>Kurtosis</p></li>
</ol>
<p>Spectral kurtosis calculates the shape of the spectral distribution around the spectral centroid. A Gauss-distribution is present with Sk = 3. Smaller values than three indicate a more flat, larger values a more sharp distribution.</p>
<div class="math notranslate nohighlight">
\[Sk = \sqrt{\left(\sum_{i=0}^N (f_i - C)^4\ A_i\right)}/Sk^4\]</div>
<ol class="arabic simple" start="10">
<li><p>Spectral flux</p></li>
</ol>
<p>Spectral flux is calculated as a second derivative of a spectrum using a central difference derivation like</p>
<div class="math notranslate nohighlight">
\[Sf = \sum_i{\frac{-2\ A_i^t + A_i^{t-1} + A_i^{t+1}}{dt^2}} \ ,\]</div>
<p>with sample rate dt.</p>
<p><strong>Jupyter notebook use</strong></p>
<p>The jupyter notebook <strong>TimbreTrack_SOM_Example</strong> shows an example use of the timbre track.</p>
<p>The timbre track need to be instantiated like</p>
<p><code class="docutils literal notranslate"><span class="pre">tt</span> <span class="pre">=</span> <span class="pre">TimbreTrack()</span></code></p>
<p>All below features are calculated like this</p>
<p><code class="docutils literal notranslate"><span class="pre">out</span> <span class="pre">=</span> <span class="pre">tt.extract('soundfile.wav')</span></code></p>
<p>The .wav file need to have a sample rate of 44.1 kHz.</p>
<p>The results can be displayed like</p>
<p><code class="docutils literal notranslate"><span class="pre">out.features</span></code></p>
<p>Below the an example result of two sets of Hip Hop musical pieces. A SOM was trained with a certain amount of features, able to cluster Chinese (red) and Western (violet) Hip Hop pieces.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/TimbreTrack_SOM_Example_HipHop.png"><img alt="TimbreTrack_SOM_Example_HipHop" src="_images/TimbreTrack_SOM_Example_HipHop.png" style="width: 577.0px; height: 575.0px;" /></a>
</div>
<p>Lit.:</p>
<p>Aures, W.: Der sensorische Wohlklang als Funktion psychoakustischer Empfindungsgroessen, [Sensory pleasing sounds as function of psychoacoustic perception parameters], Acustica 58, 282-290, 1985.</p>
<p>Bader, R.: <em>Nonlinearities and Synchronization in Musical Acoustics and Music Psychology,</em> Springer-Verlag, Berlin, Heidelberg, Current Research in Systematic Musicology, vol. 2,  2013.</p>
<p>Fastl H. &amp; Zwicker, E.: <em>Psychoacoustics. Facts and Models.</em> 3rd edition, Springer 2007.</p>
<p>von Helmholtz, H.: <em>Die Lehre von den Tonempfindungen als physiologische Grundlage fuer die Theorie der Musik [On the Sensations of tone as a physiological basis for the theory of music]</em>. Vieweg, Braunschweig 1863.</p>
<p>Ruschkowski, A. v.:  <em>Lautheit von Musik: eine empirische Untersuchung zum Einfluss von Organismusvariablen auf die Lautstaerkewahrnehmung von Musik. [Loudness of music: an empirical investigation on the impact of Organism variables on loudness perception of music.]</em> <a class="reference external" href="https://katalogplus.sub.uni-hamburg.de/vufind/Record/78110422X?rank=1">https://katalogplus.sub.uni-hamburg.de/vufind/Record/78110422X?rank=1</a> Hamburg 2013.</p>
<p>Schneider, A., von Ruschkowski, A., &amp; Bader, R.: Klangliche Rauhigkeit, ihre Wahrnehmung und Messung. In: Bader, R. (ed. / Hrsg.).: Musical Acoustics, Neurocognition and Psychology of Music / Musikalische Akustik, Neurokognition und Musikpsychologie. Hamburger Jahrbuch fuer Musikwissenschaft 25, 101-144, 2009.</p>
<p>Sethares, W.: Local consonance and the relationship between timbre and scale, J. of the Acoust. Soc. of America 94, 1218-1228, 1993.</p>
<p>Sottek, R.: Gehoergerechte Rauhigkeitsberechnung [Roughness calculation fitting perception], Tagungsbericht DAGA 94, Dresden, 1197-1200, 1994.</p>
<p>Zwicker, E. &amp; Fastl, H.: <em>Psychoacoustics. Facts and models. 2nd ed. Berlin</em>, New York, Springer, 1999.</p>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">COMSAR</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="OnlineVersion.html">Online version</a></li>
<li class="toctree-l1"><a class="reference internal" href="OnlineVersion.html#self-organizing-map-som-explained">Self-organizing map (SOM) explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="OnlineVersion.html#collections">Collections</a></li>
<li class="toctree-l1"><a class="reference internal" href="OfflineVersion.html">Online jupyter notebook version</a></li>
<li class="toctree-l1"><a class="reference internal" href="pitch_track.html">Pitch Track</a></li>
<li class="toctree-l1"><a class="reference internal" href="pitch_track.html#melody-n-grams">Melody, n-grams</a></li>
<li class="toctree-l1"><a class="reference internal" href="pitch_track.html#tonal-system">Tonal System</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Timbre Track</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="pitch_track.html" title="previous chapter">Pitch Track</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Rolf Bader.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/timbre_track.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>
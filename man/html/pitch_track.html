
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Pitch Track &#8212; COMSAR 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Timbre Track" href="timbre_track.html" />
    <link rel="prev" title="Online jupyter notebook version" href="OfflineVersion.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="pitch-track">
<h1>Pitch Track<a class="headerlink" href="#pitch-track" title="Permalink to this headline">¶</a></h1>
<p>The pitch track of the COMSAR framework extracts pitch from a soundfile, accumulates pitches into an octave, detects notes, vibrato, slurs, or melisma, determines most likely tonal systems, extracts the melody, and calculates n-gram histograms.</p>
<p>The pitch track instance should contain the frame size and hop ratio. In the example notebook 100 pitches per second are analyzed (see jupyter example notebook <strong>COMSAR_Melody_Example.jpynb</strong>)</p>
</div>
<div class="section" id="melody-n-grams">
<h1>Melody, n-grams<a class="headerlink" href="#melody-n-grams" title="Permalink to this headline">¶</a></h1>
<p>An example of a Lisu solo flute piece for pitch and melody extraction is shown in the figure below (ESRA index 626, <a class="reference external" href="https://vimeo.com/showcase/5259277/video/278497941">https://vimeo.com/showcase/5259277/video/278497941</a>).</p>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="_images/PitchMelody_Lisu_626.png"><img alt="Melody Lisu flute solo" src="_images/PitchMelody_Lisu_626.png" style="width: 651.0px; height: 409.0px;" /></a>
<p class="caption"><span class="caption-text">Example of pitch and melody extraction using Lisu flute solo, ESRA index 626. The analysis has five stages of abstraction. 1) pitches are calculated over the whole piece. From pitch values to melodies: 2) top left: Pitch contours of detected notes, 3) top right) pitch contour of notes allowed for n-grams (melodies), 4) bottom left: mean pitches of allowed notes from plot 2) still showing grace-notes, 5) bottom right: mean pitches of notes allowed for n-grams.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p><strong>In a first analysis stage’</strong>, the original sound files are analyzed with respect to <strong>pitch</strong>, where n pitch values are calculated per second, determined by the args used in the instantiation of the pitch track. Pitch analysis is performed using the autocorrelation method, where the first peak of the autocorrelation function determines the periodicity of the pitch.</p>
<p>For frequencies below about 50 Hz the result of the autocorrelation is not precise. In nearly all cases, the amount of period cycles the autocorrelation integrates over is not an integer, but a fraction of one period is integrated over at the end of the sound. With higher frequencies, this is not considerable. Still, with low frequencies, due to the few sound period cycles the autocorrelation integrates over, this leads to incorrect results. Here an algorithm is used to compensate for overintegration. For frequencies above 1.7 kHz, results again are not correct due to the limited sample frequency, allowing only certain periods. Here, oversampling is performed to compensate for incorrectness. Still, in most cases, melody falls into the human singing range, and, therefore, the algorithm does not need these corrections.</p>
<p>The pitches are transferred into cent, using a fundamental frequency of f0 to be determined in the args (f0 = 27.5 Hz is recommended for sub contra A) in noctaves above f0 (noctaves = 8 is recommended), and a cent precision dcent (dcent = 1 cent is recommended).</p>
<p><strong>The second abstraction stage</strong> uses an agent-based approach, where <strong>musical events, notes, grace-notes, slurs, melismas, etc.</strong> are detected. The agent follows the cent values from the start of each musical piece and concatenates adjacent cent values according to two constraints, a minimum length (minlen) a pitch event needs to have and a maximum allowed cent deviation (mindev). So, e.g., with pitch track instantiated using 100 pitch values per second, a minlen = 3 is 30 ms minimum note length, and mindev = 60 his allows for including vibrato and pitch glides within about one semitone, often found in vocal and some instrumental music. Lowering the allowed deviation often leads to the exclusion of pitches, which often have quite strong deviations. As an example, an excerpt of 23 seconds of a Lisu solo flute piece is shown in the figure below on the top left. Some pitches show a quite regular periodicity, some are slurs or grace-notes.</p>
<p><strong>The third abstraction stage</strong> determines <strong>single pitches</strong> for each detected event by taking the strongest value of a pitch histogram. As can be seen in the top left figure, often pitches are stable, only to end in some slur in the end. Therefore, taking the mean of these pitches would not represent the main pitch. Using the maximum of a histogram, on the other side, detects the pitch most frequency occurring during the event. On the bottom left, this is performed and can be compared to the top left plot. When listening to the piece, this representation seem to contain still too many pitch events. So,. e.g., the events around 6000 cent are clearly perceived as notes. Still, those small events preceding around 6200 cent are heard as grace-notes. Therefore, to obtain a melody without grace-notes, a fourth stage needs to be performed.</p>
<p><strong>In a fourth stage</strong>, pitch events are selected using three constraints to allow for <strong>n-gram construction</strong>.  n-grams have shown to represent melodies stable and robust in terms of melody identification, like e.g., in query-by-humming tasks. Here, n adjacent notes are clustered in an n-gram. A musical piece, therefore, has N-n n-grams, where N is the amount of notes in the piece. The n-grams are not constructed from the notes themselves, but from the intervals between the notes. Therefore, a 3-gram has two intervals. Also, the n-grams are sorted in 12-tone just intonation. Therefore, each interval is sorted into its nearest pitch-class. Further implementations might include using tonal systems as pitch classes. Usuall,y 2-grams or 3-grams are used, sometimes up to 5-grams. All n-grams present in a piece are collected in a histogram, where, in the present study, the nngram most frequent n-grams (ten in this case) are collected into a feature vector to be fed into the machine learning algorithm.</p>
<p>So adjacent notes qualifying for n-gram inclusion need to be such to exclude grace-notes, slurs, etc. This is obtained by demanding the notes to have a certain length (minnotelength in amount of analysis frames), in the example below 100 ms is used (minnotelength = 10 as pitch track instantiation with 100 pitches per second was performed). Additionally, a lower (ngcentmin) and upper (ngcentmax) limit for adjacent note intervals is applied. The lower limit is 0 cent in the example to allow for tone repetition. The upper limit is set to +-1200 cent here, so two octaves, most often enough for traditional music. This does not mean that traditional pieces do not have larger intervals, as e.g., expected in jodeling. Still, such techniques are not used in the present music corpus, and even when present, they are not expected to be more frequent than smaller intervals. In the top right figure, we see the pitch contours for all allowed notes used in n-gram calculation. Indeed, all grace-notes are gone.</p>
<p><strong>In a last step</strong>, shown in the bottom right plot, the pitches of each event are again taken as the maximum of the histogram of each event. Now following the musical piece aurally, the events represent the <strong>melody</strong>. These notes are used for the n-gram vector.</p>
<p>An example of a trained SOM using musical pieces of Kachin ethnic group, northern Myanmar with Uyghur musical pieces from Xinjiang, western China is shown below. The map is trained with 5-grams. Most Uyghur pieces are located at the lower blue region, mainly because of the frequent tone repetitions of Uyghur music compared to Kachin songs.</p>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="_images/ngram_5_noteminlen_10_BaderUyghur.png"><img alt="5-gram SOM" src="_images/ngram_5_noteminlen_10_BaderUyghur.png" style="width: 1769.5px; height: 1795.5px;" /></a>
<p class="caption"><span class="caption-text">Trained 5-gram SOM, comparing Kachin and Uyghur musical pieces. Most Uyghur pieces are located in the lower blue region due to enhanced note repetition found in Uyghur music.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="tonal-system">
<h1>Tonal System<a class="headerlink" href="#tonal-system" title="Permalink to this headline">¶</a></h1>
<p>Tonal systems are normally understood as a small set of cent values. So a 7-tone scale has seven-pitch values. Still, depending on musical performance, determining a tonal system is more or less complex. Articulation in singing often leads to a large variation in pitch. The same might hold for lutes, guitars, violins, or wind instruments. Percussion instruments have a much more straight pitch. till, they often are inharmonic, and, therefore, a pitch might not even be perceived.</p>
<p>Therefore the MIR tool for investigating tonal systems takes tonal systems as an accumulation of pitch values over mainly single-voiced musical pieces compressed within one octave with a precision of one cent. An autocorrelation algorithm determines the pitch for n time frames per second, the one already shown in the melody section above.</p>
<p>In a second stage, pitch events are detected, again as discussed above. All pitches of the detected musical events are then accumulated in dcent values (dcent = 1 is recommended), starting again from f0 in noctaves. To also include melismas and slurs in the calculation, the tonal system is derived from all pitch values in the musical event (top left plot of above figure). If the tonal system should only be constructed from pitch events with a very constant pitch, the mindev parameter needs to be small.</p>
<p>The strongest frequency maxf is then taken as the fundamental of the tonal system. In the tonal system plots shown below, this lowest cent is not shown, as it often overwhelms the other accumulated cent values.</p>
<p>In a last step, using the largest accumulated pitch as fundamental of the tonal system, all accumulated cents in noctaves are mirrored into one octave. When a precision of one cent was used, the input feature vector to the SOM has a length of 1200, reflecting 1200 cent in one octave.</p>
<div class="figure align-default" id="id3">
<a class="reference internal image-reference" href="_images/TonalSystem_Examples.png"><img alt="Tonal System Examples" src="_images/TonalSystem_Examples.png" style="width: 609.5px; height: 619.5px;" /></a>
<p class="caption"><span class="caption-text">Three examples of tonal systems as calculated from a sound file (left column) and as a vector on the neural map on the location the musical pieces fits best (right column). Top: Uygur Rock/Pop piece by Qetik, Middel: Kachin flute solo piece, Bottom: Lisu flute solo piece.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The outputs are</p>
<blockquote>
<div><ul class="simple">
<li><p>accumulated cent values over noctaves</p></li>
<li><p>accumulated cent values within one octave</p></li>
<li><p>Names of the ten best-matchin tonal systems</p></li>
<li><p>Cent values of the best-matching tonal systems</p></li>
<li><p>Correlation of each cent value in all best-matching scale to estimate the salience of each note to the overall large correlation between theoretical scale and calculated values</p></li>
<li><p>Overall correlation of the best-matching scales</p></li>
</ul>
</div></blockquote>
<p>The tonal systems used for comparison are taken from a set of scales: <a class="reference external" href="https://www.flutopedia.com/scales.htm">https://www.flutopedia.com/scales.htm</a>. The list contains over 900 scales. Therefore, matches might not meet expectations. Reduced lists fitting special intrests will be developed in the future.</p>
<p>Below, a trained SOM with tonal systems of the Kachin vs. Uyghur music case is shown below. Both ethnic groups are clustered, where Uyghur pieces on the lower left are nearer to just intonation, while Kachin shows more deviating pitches.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/TonalSystem_UyghurLisu_RawangShanBama.png"><img alt="5-gram SOM" src="_images/TonalSystem_UyghurLisu_RawangShanBama.png" style="width: 424.5px; height: 431.0px;" /></a>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">COMSAR</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="OnlineVersion.html">Online version</a></li>
<li class="toctree-l1"><a class="reference internal" href="OnlineVersion.html#self-organizing-map-som-explained">Self-organizing map (SOM) explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="OnlineVersion.html#collections">Collections</a></li>
<li class="toctree-l1"><a class="reference internal" href="OfflineVersion.html">Online jupyter notebook version</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pitch Track</a></li>
<li class="toctree-l1"><a class="reference internal" href="#melody-n-grams">Melody, n-grams</a></li>
<li class="toctree-l1"><a class="reference internal" href="#tonal-system">Tonal System</a></li>
<li class="toctree-l1"><a class="reference internal" href="timbre_track.html">Timbre Track</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="OfflineVersion.html" title="previous chapter">Online jupyter notebook version</a></li>
      <li>Next: <a href="timbre_track.html" title="next chapter">Timbre Track</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Rolf Bader.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/pitch_track.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>